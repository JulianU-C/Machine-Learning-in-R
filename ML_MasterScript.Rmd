---
title: "Midterm"
author: "JUC"
date: "`r Sys.Date()`"
output: html_document
---
#### Anova
```{r}
setwd("C:/Users/Owner/Desktop/STAT627")
homes <- read.csv("HOME_SALES.csv")
attach(homes)


reg <- lm(SALES_PRICE ~ FINISHED_AREA + BEDROOMS)
plot(FINISHED_AREA, SALES_PRICE)

abline(reg)
reduced <- lm(SALES_PRICE ~ QUALITY + FINISHED_AREA)
full <- lm(SALES_PRICE ~ QUALITY * FINISHED_AREA)
# example this is not actually the full model

anova(reduced, full)

# Pr(>F) shows that the interaction terms are significant, so we should keep them in our model
```


#### Logistic regression
```{r, message=FALSE}
library(ISLR2)
attach(Weekly)
logreg <- glm(Direction ~ Volume + Lag1 + Lag2 
              + Lag4 + Lag5, family = "binomial")

# Confusion matrix
phat <- predict(logreg, newdata = Weekly, type = "response")
yhat <- rep("Down", length(phat))
yhat[phat > 0.5] <- "Up"

table(truth = Weekly$Direction, predcited = yhat)
mean(Weekly$Direction == yhat)
mean(Weekly$Direction == "Up")
mean(yhat == "Up")

# Create an ROC curve
TPR = rep(0,100); FPR = rep(0,100);
fit = glm( Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, family="binomial")
Prob = predict( fit, type="response" )
for (k in 1:100){
 Yhat = ( Prob > k/100 )
 TPR[k] = sum( Yhat == 1 & Direction == "Up" ) / sum( Direction == "Up" )
 FPR[k] = sum( Yhat == 1 & Direction == "Down" ) / sum( Direction == "Down" )
 }

plot(FPR, TPR, xlab="False positive rate",
     ylab="True positive rate", main="ROC curve")

lines(FPR,TPR)
```
We correctly predicted 54 weeks when market went down and 557 weeks when market went up, and overall, our correct classification rate is 56% and the error rate is 44%, which is only a little better than a coin toss. Our model classification rule is too optimistic, it predicts positive market 90.6% of the time whereas it only happens 55.6% of the time.
 
 
#### Discriminant analysis
```{r, message=FALSE}
library(ISLR2)
attach(Weekly)
Weekly_training = Weekly[ Year <= 2008, ] 
Weekly_testing = Weekly[ Year > 2008, ] 

# LDA model
library(MASS)
lda.fit = lda(Direction ~ Lag2, data=Weekly_training) 

# Use training model to predict classification fo test data
Direction_Predicted_LDA = predict(lda.fit, data.frame(Weekly_testing))$class 
table( Weekly_testing$Direction, Direction_Predicted_LDA ) 
mean( Weekly_testing$Direction == Direction_Predicted_LDA ) # class. rate

# QDA model
qda.fit = qda(Direction ~ Lag2, data=Weekly_training) 
Direction_Predicted_QDA = predict(qda.fit, data.frame(Weekly_testing))$class 
table( Weekly_testing$Direction, Direction_Predicted_QDA ) 
mean( Weekly_testing$Direction == Direction_Predicted_QDA ) # class. rate
```

#### KNN
```{r, message=FALSE}
library(ISLR2)
attach(Weekly)

X.train = matrix( Lag2[ Year <= 2008 ], sum(Year <= 2008), 1 ) 
X.test = matrix( Lag2[ Year >= 2009 ], sum(Year >= 2009), 1 ) 
Y.train = Direction[ Year <= 2008 ] 
Y.test = Direction[ Year >= 2009 ]

# KNN model3
library(class)
knn.fit = knn( X.train, X.test, Y.train, 1 ) # K = 1 neighbor
knn.fit4 = knn( X.train, X.test, Y.train, 4 ) # K = 4 neighbors
table( Y.test, knn.fit ) 
mean( Y.test == knn.fit ) # class. rate
mean( Y.test == knn.fit4 ) # class. rate
```

#### Cross validation methods
```{r, message=FALSE}
library(ISLR2)
attach(Auto)

## Validation set
n <- nrow(Auto)
train <- sample(n, 250) # sample 100 random obs
test <- Auto[-train, ]

linreg <- lm(mpg ~ weight + acceleration + horsepower, subset = train)

# prediction - predicted mpg values for the test data
Yhat <- predict(linreg, newdata = test)

## LOOCV
library(boot)
attach(Auto)
# linear model has to be glm
linreg <- glm(mpg ~ weight + acceleration + horsepower, data = Auto)
LOOCV <- cv.glm(data = Auto, linreg)
LOOCV$delta # MSE and adjusted MSE

## K fold CV
library(boot)
attach(Auto)
linreg <- glm(mpg ~ weight + acceleration + horsepower, data = Auto)
regKfold <- cv.glm(Auto, linreg, K=10)
regKfold$delta # MSE and adjusted MSE

# APPLICATION: tune the degree of a polynomial
set.seed(13)
MSE <- numeric(12)

for (k in 1:12) {
  reg <- glm(mpg ~ weight + acceleration + poly(horsepower, k), data = Auto)
  regKfold <- cv.glm(Auto, reg, K=10)
  MSE[k] = regKfold$delta[2] # we'll use the adjusted delta
}

plot(MSE, type = "l")
which.min(MSE) # looks like 7th degree polynomial is the best fit
```

#### Jackknife
```{r, message=FALSE, warning=FALSE}
#variance function
variance.fn <- function(X){ return(mean((X - mean(X))^2))}

attach(Auto)
X = Auto$mpg
variance.fn(X) # biased  sample variance
var(X) # R computes an unbiased one

# jackknife
library(bootstrap)
JK <- jackknife(x = X, theta = variance.fn)
JK$jack.bias

# jackknife estimator = bias- original estimate
variance.fn(X) - JK$jack.bias 

# tank example
# Example from tanks
X = c(23, 76, 71, 125, 239)
max.fn = function(X){ return(max(X))}

JK <- jackknife(X, max.fn)
JK$jack.bias
(thetaJK <- max(X) - JK$jack.bias) # jackknife estimate
```

#### Bootstrap
Using WW2 tank example data
```{r, message=FALSE, warning=FALSE}
library(boot)

X <- c(23, 51, 76, 125, 239)
# max function
max.fn <- function(X, subsample) {return(max(X[subsample]))}
# here's also a mean and median function
# mean.fn <-  function( X, sample ){ return( mean( X[sample] ) ) }
# median.fn = function( X, sample ){ return( median( X[sample] ) ) }

(Boot <- boot(X, max.fn, R=1000))

# our original estimator of the max
Boot$t0

# 95% confidence interval for max tank number, it's the interval b/w two quantiles
quantile(Boot$t, c(0.25, 0.975))
```

#### Another bootstrap example for vaccine efficacy
```{r, message=FALSE, warning=FALSE}
# Data: X = infection, V = vaccination
n1 = 14134
n0 = 14073
x1 = 11 
x0 = 185
V = c(rep(1,n1),rep(0,n0))
X = rep(0,n1+n0)
X[1:x1] = 1
X[(n1+1):(n1+x0)] = 1

Data = data.frame(X,V)

eff.fn = function(data,Z){
  x = data[Z,1]
  v = data[Z,2]
  p1 = sum( x==1 & v==1 )/sum( v==1 )
  p0 = sum( x==1 & v==0 )/sum( v==0 )
  return( 1-p1/p0 ) 
  }

eff.fn(Data,)


library(boot)
BT = boot( Data, eff.fn, R=100 )
BT # original = efficacy estimate
quantile(BT$t, c(0.025, 0.975)) # confidence interval
```

#### Variable selection
VIFj = 1/ 1-Rj2 - 
```{r, message=FALSE, warning=FALSE}
library(ISLR2)
attach(Boston)
# Variance inflation factors(VIF)
library(car)
reg <- lm( medv ~ ., data=Boston ) 
vif(reg)

## Variable selection
library(ISLR2) 
attach(College) 
n <- nrow(College) 
Z <- sample(n,n/2) 
# Use option nvmax to increase the max number of variables used 
library(leaps)
reg.fit = regsubsets( Apps ~ ., data=College[Z,], nvmax=ncol(College) ) 
summary(reg.fit) 

# adjusted R^2 should be as high as possible, BIC as low as possible,  
# and Cp approx = p 
which.min(summary(reg.fit)$bic) 
which.max(summary(reg.fit)$adjr2) 
summary(reg.fit)$cp  
# BIC choose a model with 7 variables, adjusted R2 with 11, and Cp is close to p for p>8

## Forward selection
null = lm( Apps ~ 1, data=College[Z,] ) 
full = lm( Apps ~ ., data=College[Z,] ) 
model.selection = step( null, list( lower=null, upper=full ), direction="forward" )
model.selection 

# We get a model with 11 variables. Now, evaluate its prediction accuracy by cross-validation
Yhat = predict( model.selection, College[-Z,] ) 
mean( (Yhat - Apps[-Z])^2 ) # predicted MSE
#The least squares regression chooses 11 variables and results in the prediction mean-squared error of 1,145,624.

```
#### Ridge regression 
```{r, message=FALSE, warning=FALSE}
library(ISLR2) 
attach(College) 
n <- nrow(College) 
Z <- sample(n,n/2)

Ridge <- lm.ridge( Apps ~ ., data=College, lambda=seq(0,1000,1) ) 
select(Ridge) 

# Narrow the range of lambda 
Ridge <- lm.ridge( Apps ~ ., data=College, lambda=seq(0,10,0.1) ) 
select(Ridge) 

# Variance of MSE is so high that even a slightly different random split into the training and testing data may result in an entirely different best value of Î».


# Now, find predicted values (multiplying the design matrix X by the vector of coefficients beta) and calculate the prediction MSE from the testing data.
X = model.matrix(reg) 
beta = coef(Ridge) 
Yhat = X %*% beta 
MSE = mean((Yhat[-Z] - College$Apps[-Z])^2) 
MSE 
# Ridge regression uses all 17 variables and results in prediction error of 1,023,741. 
```

#### Lasso
```{r, message=FALSE, warning=FALSE}


reg = lm( Apps ~., data=College ) 
X = model.matrix(reg) 
cv.lasso = cv.glmnet( X[Z,], Apps[Z], alpha=1, lambda=seq(0,40,0.1) ) 
cv.lasso$lambda.min 

cv.lasso = cv.glmnet( X[Z,], Apps[Z], alpha=1, lambda=seq(0,20,0.001) ) 
cv.lasso$lambda.min 

lasso = glmnet( X[Z,], Apps[Z], alpha=1, lambda=cv.lasso$lambda.min ) 
coef(lasso) 
                  
 
Yhat = predict( lasso, cv.lasso$lambda.min, newx = X[-Z,] ) 
mean( (Yhat - Apps[-Z])^2 ) 
# Lasso eliminates P.Undergrad and Books and uses 15 variables. It results in prediction error of 1,490,239. 
```


